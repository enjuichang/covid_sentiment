{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Statistical Model\nThis is the code for the training the statistical models.\n\n### Structure\n- Package Setup\n- Preprocessing for labels\n- Tokenization\n- Model Training\n- Metrics",
   "metadata": {
    "tags": [],
    "cell_id": "00000-82490aa3-4853-4f72-b547-163f8a28833a",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Setup\nHere, I setup the packages and imported the data needed for model training.\n\n**Downloaded Packages**\n1. SpaCy English library\n2. Contextual Spell Check\n\n\n**Imported Packages**\n1. Pandas\n2. SpaCy\n3. Scikit-learn\n4. string\n5. re",
   "metadata": {
    "tags": [],
    "cell_id": "00001-f8db77ad-cf41-4a66-ba89-c506a9005552",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b816a265",
    "execution_start": 1636064511546,
    "execution_millis": 10968,
    "cell_id": "00002-c66a6dad-19a0-4cb5-93cc-5483be35b121",
    "deepnote_cell_type": "code"
   },
   "source": "# Installation of packages and embedding\n!python -m spacy download en_core_web_sm\n\n# Install package for Contextual Spell Check\n!pip install contextualSpellCheck\n!pip install ipywidgets --upgrade",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "2021-11-04 22:21:53.810310: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2021-11-04 22:21:53.810350: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nCollecting en-core-web-sm==3.1.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n     |████████████████████████████████| 13.6 MB 16.9 MB/s            \n\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from en-core-web-sm==3.1.0) (3.1.3)\nRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.10.0.2)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\nRequirement already satisfied: srsly<3.0.0,>=2.4.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\nRequirement already satisfied: packaging>=20.0 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (21.0)\nRequirement already satisfied: numpy>=1.15.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.19.5)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.4)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.0)\nRequirement already satisfied: jinja2 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.2)\nRequirement already satisfied: pathy>=0.3.5 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\nRequirement already satisfied: thinc<8.1.0,>=8.0.9 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.10)\nRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.26.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.62.3)\nRequirement already satisfied: setuptools in /root/venv/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (58.1.0)\nRequirement already satisfied: zipp>=0.5 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.6.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\nRequirement already satisfied: smart-open<6.0.0,>=5.0.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.2.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.26.7)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.10.8)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.7)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.1)\nRequirement already satisfied: importlib-metadata in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.8.1)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fd0006ae",
    "execution_start": 1636064522522,
    "execution_millis": 2845,
    "cell_id": "00003-fb5eda1b-0b9e-4712-a515-3170828c273e",
    "deepnote_cell_type": "code"
   },
   "source": "# Import packages\nimport pandas as pd\nimport spacy",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "I imported the data here.",
   "metadata": {
    "tags": [],
    "cell_id": "00004-44b766b4-1895-4628-b4d7-1f01cdf71de4",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "162d9b1c",
    "execution_start": 1636064525367,
    "execution_millis": 242,
    "cell_id": "00005-c4334d04-d60e-4d4f-b88e-cebc54b5ac64",
    "deepnote_cell_type": "code"
   },
   "source": "# Import dataset and pandas\nraw_trainDF = pd.read_csv(\"/work/data/coronavirus_tweet_raw/Corona_NLP_train.csv\")\nraw_testDF = pd.read_csv(\"/work/data/coronavirus_tweet_raw/Corona_NLP_test.csv\")\nraw_trainDF.head()",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 3,
     "data": {
      "application/vnd.deepnote.dataframe.v2+json": {
       "row_count": 5,
       "column_count": 6,
       "columns": [
        {
         "name": "UserName",
         "dtype": "int64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "3799",
          "max": "3803",
          "histogram": [
           {
            "bin_start": 3799,
            "bin_end": 3799.4,
            "count": 1
           },
           {
            "bin_start": 3799.4,
            "bin_end": 3799.8,
            "count": 0
           },
           {
            "bin_start": 3799.8,
            "bin_end": 3800.2,
            "count": 1
           },
           {
            "bin_start": 3800.2,
            "bin_end": 3800.6,
            "count": 0
           },
           {
            "bin_start": 3800.6,
            "bin_end": 3801,
            "count": 0
           },
           {
            "bin_start": 3801,
            "bin_end": 3801.4,
            "count": 1
           },
           {
            "bin_start": 3801.4,
            "bin_end": 3801.8,
            "count": 0
           },
           {
            "bin_start": 3801.8,
            "bin_end": 3802.2,
            "count": 1
           },
           {
            "bin_start": 3802.2,
            "bin_end": 3802.6,
            "count": 0
           },
           {
            "bin_start": 3802.6,
            "bin_end": 3803,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "ScreenName",
         "dtype": "int64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "48751",
          "max": "48755",
          "histogram": [
           {
            "bin_start": 48751,
            "bin_end": 48751.4,
            "count": 1
           },
           {
            "bin_start": 48751.4,
            "bin_end": 48751.8,
            "count": 0
           },
           {
            "bin_start": 48751.8,
            "bin_end": 48752.2,
            "count": 1
           },
           {
            "bin_start": 48752.2,
            "bin_end": 48752.6,
            "count": 0
           },
           {
            "bin_start": 48752.6,
            "bin_end": 48753,
            "count": 0
           },
           {
            "bin_start": 48753,
            "bin_end": 48753.4,
            "count": 1
           },
           {
            "bin_start": 48753.4,
            "bin_end": 48753.8,
            "count": 0
           },
           {
            "bin_start": 48753.8,
            "bin_end": 48754.2,
            "count": 1
           },
           {
            "bin_start": 48754.2,
            "bin_end": 48754.6,
            "count": 0
           },
           {
            "bin_start": 48754.6,
            "bin_end": 48755,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "Location",
         "dtype": "object",
         "stats": {
          "unique_count": 3,
          "nan_count": 2,
          "categories": [
           {
            "name": "London",
            "count": 1
           },
           {
            "name": "2 others",
            "count": 2
           },
           {
            "name": "Missing",
            "count": 2
           }
          ]
         }
        },
        {
         "name": "TweetAt",
         "dtype": "object",
         "stats": {
          "unique_count": 1,
          "nan_count": 0,
          "categories": [
           {
            "name": "16-03-2020",
            "count": 5
           }
          ]
         }
        },
        {
         "name": "OriginalTweet",
         "dtype": "object",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "categories": [
           {
            "name": "@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8",
            "count": 1
           },
           {
            "name": "advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order",
            "count": 1
           },
           {
            "name": "3 others",
            "count": 3
           }
          ]
         }
        },
        {
         "name": "Sentiment",
         "dtype": "object",
         "stats": {
          "unique_count": 3,
          "nan_count": 0,
          "categories": [
           {
            "name": "Positive",
            "count": 3
           },
           {
            "name": "Neutral",
            "count": 1
           },
           {
            "name": "Extremely Negative",
            "count": 1
           }
          ]
         }
        },
        {
         "name": "_deepnote_index_column",
         "dtype": "int64"
        }
       ],
       "rows_top": [
        {
         "UserName": 3799,
         "ScreenName": 48751,
         "Location": "London",
         "TweetAt": "16-03-2020",
         "OriginalTweet": "@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.c…",
         "Sentiment": "Neutral",
         "_deepnote_index_column": 0
        },
        {
         "UserName": 3800,
         "ScreenName": 48752,
         "Location": "UK",
         "TweetAt": "16-03-2020",
         "OriginalTweet": "advice Talk to your neighbours family to exchange phone numbers create contact list with phone numb…",
         "Sentiment": "Positive",
         "_deepnote_index_column": 1
        },
        {
         "UserName": 3801,
         "ScreenName": 48753,
         "Location": "Vagabonds",
         "TweetAt": "16-03-2020",
         "OriginalTweet": "Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 …",
         "Sentiment": "Positive",
         "_deepnote_index_column": 2
        },
        {
         "UserName": 3802,
         "ScreenName": 48754,
         "Location": "nan",
         "TweetAt": "16-03-2020",
         "OriginalTweet": "My food stock is not the only one which is empty...\r\r\n\r\r\nPLEASE, don't panic, THERE WILL BE ENOUGH …",
         "Sentiment": "Positive",
         "_deepnote_index_column": 3
        },
        {
         "UserName": 3803,
         "ScreenName": 48755,
         "Location": "nan",
         "TweetAt": "16-03-2020",
         "OriginalTweet": "Me, ready to go at supermarket during the #COVID19 outbreak.\r\r\n\r\r\nNot because I'm paranoid, but bec…",
         "Sentiment": "Extremely Negative",
         "_deepnote_index_column": 4
        }
       ],
       "rows_bottom": null
      },
      "text/plain": "   UserName  ScreenName   Location     TweetAt  \\\n0      3799       48751     London  16-03-2020   \n1      3800       48752         UK  16-03-2020   \n2      3801       48753  Vagabonds  16-03-2020   \n3      3802       48754        NaN  16-03-2020   \n4      3803       48755        NaN  16-03-2020   \n\n                                       OriginalTweet           Sentiment  \n0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n1  advice Talk to your neighbours family to excha...            Positive  \n2  Coronavirus Australia: Woolworths to give elde...            Positive  \n3  My food stock is not the only one which is emp...            Positive  \n4  Me, ready to go at supermarket during the #COV...  Extremely Negative  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3799</td>\n      <td>48751</td>\n      <td>London</td>\n      <td>16-03-2020</td>\n      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3800</td>\n      <td>48752</td>\n      <td>UK</td>\n      <td>16-03-2020</td>\n      <td>advice Talk to your neighbours family to excha...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3801</td>\n      <td>48753</td>\n      <td>Vagabonds</td>\n      <td>16-03-2020</td>\n      <td>Coronavirus Australia: Woolworths to give elde...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3802</td>\n      <td>48754</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>My food stock is not the only one which is emp...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3803</td>\n      <td>48755</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>Me, ready to go at supermarket during the #COV...</td>\n      <td>Extremely Negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1734b6f0",
    "execution_start": 1636064525562,
    "execution_millis": 0,
    "cell_id": "00006-a1b712c2-6254-4b2f-a504-3bb9619c6db5",
    "deepnote_cell_type": "code"
   },
   "source": "# Copy the values of the data for further uses\ntrainDF = raw_trainDF\ntestDF = raw_testDF",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1635790068946,
    "execution_millis": 2,
    "cell_id": "00007-e24030d7-294d-498b-93ef-04eb30cc5397",
    "deepnote_cell_type": "visualization"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Reorganized the data\nDue to the observation during the EDA process, I decided to concatenate the dataset and resplit them.",
   "metadata": {
    "tags": [],
    "cell_id": "00015-fbdd4eff-bdfe-4ea3-8cd0-853b6260a38f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "aab1244e",
    "execution_start": 1636064525563,
    "execution_millis": 625,
    "allow_embed": "code_output",
    "cell_id": "00016-c568566c-0e9f-464d-8fed-63c6b2d260b9",
    "deepnote_cell_type": "code"
   },
   "source": "# Train test split\nfrom sklearn.model_selection import train_test_split\n\n# Concat the two datasets and split them\nallDF = pd.concat((trainDF, testDF), ignore_index=True)\n\n# Sample dataset due to the large size\nallDF = allDF.sample(frac=0.5).reset_index(drop=True)\n\n# Split the train, test, validation set\ntrainDF, testDF = train_test_split(allDF, test_size = 0.2)\ntestDF, validDF = train_test_split(testDF, test_size = 0.2)\n\n# Print values\nprint(\"Train:\",len(trainDF), \"Test:\", len(testDF),\"Valid:\", len(validDF))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Train: 17982 Test: 3596 Valid: 900\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Preprocessing\nThis is the first part of the preprocessing where we create all parts of the pipeline except for the models.\n\n### Structure\n- Label encode\n- Contextual Spell Check\n- Tokenizer\n- Pipeline",
   "metadata": {
    "tags": [],
    "cell_id": "00022-19e3af6c-83e5-4f5c-963c-3a6a760a994d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "![Preprocess Image](../images/preprocess_tra.png)\n",
   "metadata": {
    "tags": [],
    "cell_id": "00011-8120952a-0f82-4337-8acb-d59991d54d82",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Encode the Labels\nI used `scikit-learn` to encoder the labels from 1 to 5.",
   "metadata": {
    "tags": [],
    "cell_id": "00024-46d731db-5317-4f81-aeb5-19e026c2c217",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e1346d87",
    "execution_start": 1636064528495,
    "execution_millis": 5,
    "cell_id": "00025-5ff74687-37af-4981-bd0d-7d77c52e639b",
    "deepnote_cell_type": "code"
   },
   "source": "# Label Encoder for classes in sentiment \nfrom sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\ntrainDF.encoded_sentiment = encoder.fit_transform(trainDF.Sentiment)\ntrainDF.encoded_sentiment\n\nencoder = LabelEncoder()\ntestDF.encoded_sentiment = encoder.fit_transform(testDF.Sentiment)\ntestDF.encoded_sentiment",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n  \"\"\"\n/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n  if __name__ == '__main__':\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 6,
     "data": {
      "text/plain": "array([1, 4, 3, ..., 0, 2, 2])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Spelling Correction\nI use `contextualSpellCheck` package for fuzzy matching and correcting typos.",
   "metadata": {
    "tags": [],
    "cell_id": "00026-184d54d7-9ee2-4bca-98fe-0264efcf2806",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1d5f10ef",
    "execution_start": 1636064530953,
    "execution_millis": 11402,
    "cell_id": "00027-283d43cf-21fb-494a-99cc-44a507302596",
    "deepnote_cell_type": "code"
   },
   "source": "# Correct Spelling\nimport contextualSpellCheck\nimport ipywidgets\n\n# Download spacy English library\nnlp = spacy.load('en_core_web_sm')\n\n# Add contextual spellchecker to the pipeline\n\nnlp.add_pipe(\"contextual spellchecker\", config={\"max_edit_dist\": 5})    \n\n# create token of text\nsample_text = trainDF.OriginalTweet[0]\ndoc = nlp(sample_text)\n\nprint(doc._.outcome_spellCheck)",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e51dc927726420b9b15cd3bcdf8587d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "731a0778f53143698af4a651089a2ec8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83b9968379954d5795adb7fe0395d166"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "adb6e119f8dd415499f9a5389fd9f356"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c7aaafc20834f9db5290dbd3660a481"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "Deals to buy in practices will be less upfront and more backended. COVID-19 throws a curve ball at  RIA M&amp;A market, gut-punching valuations and causing fence-runners to resolve to get the hell out, but the prices hold their own https://t.co/R4js4NmcSE\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Tokenization Pipeline\nThis is the entire pipeline for tokenization, including removing urls, punctuations, spelling correction, and lemmatization.",
   "metadata": {
    "tags": [],
    "cell_id": "00028-fcaf94f2-6f6c-4846-a91e-e6e9324d3285",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fd876a85",
    "execution_start": 1636064547584,
    "execution_millis": 3466,
    "allow_embed": "code_output",
    "cell_id": "00029-d8128128-388e-48fc-ab92-fb42d2a7c11a",
    "deepnote_cell_type": "code"
   },
   "source": "import string\nimport re\n\n# Load the English library from SpaCy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Add contextual spell check to pipeline\nnlp.add_pipe(\"contextual spellchecker\", config={\"max_edit_dist\": 5})    \n\n# Create list of punctuation marks\npunctuations = string.punctuation\n\n# Create list of stopwords from spaCy\nstopwords = spacy.lang.en.stop_words.STOP_WORDS\n\n# Remove URLs\ndef remove_urls(text):\n    text = re.sub(r\"\\S*https?:\\S*\", \"\", text, flags=re.MULTILINE)\n    return text\n\n# Creat tokenizer function\ndef spacy_tokenizer(sentence):\n    # Create token object from spacy\n    #docs = nlp(sentence)\n    tokens = nlp(sentence)\n\n    # Correct spelling\n    #tokens = docs._.outcome_spellCheck\n    #tokens = nlp(tokens)\n\n    # Lemmatize each token and convert each token into lowercase\n    tokens = [word.lemma_.lower().strip() if word.lemma_ != \"PROPN\" else word.lower_ for word in tokens]\n    \n    # Remove stopwords\n    tokens = [word for word in tokens if word not in stopwords and word not in punctuations]\n    \n    # Remove links\n    tokens = [remove_urls(word) for word in tokens]\n    \n    # return preprocessed list of tokens\n    return tokens\n\nspacy_tokenizer(sample_text)\n\n",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 8,
     "data": {
      "text/plain": "['deal',\n 'buy',\n 'indy',\n 'practice',\n 'upfront',\n 'backende',\n 'covid-19',\n 'throw',\n 'curve',\n 'ball',\n 'ria',\n 'm&amp;a',\n 'market',\n 'gut',\n 'punching',\n 'valuation',\n 'cause',\n 'fence',\n 'sitter',\n 'resolve',\n 'hell',\n 'q1',\n 'price',\n 'hold',\n '']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Bag-of-words Model\nThis is the code for bag-of-words model using `scikit learn`'s `CountVectorizer`.",
   "metadata": {
    "tags": [],
    "cell_id": "00030-30263491-a93c-4bdf-8513-d4cf4ef14a94",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2391edbe",
    "execution_start": 1636064551047,
    "execution_millis": 68,
    "allow_embed": "code_output",
    "cell_id": "00031-ee28391a-a909-4289-aaab-9c9ffae12e21",
    "deepnote_cell_type": "code"
   },
   "source": "# Bag-of-words data transformation\nfrom sklearn.feature_extraction.text import CountVectorizer\nbow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Pipeline\n\nThe following cell contains the entire preprocessing pipeline from tokenization to training models.",
   "metadata": {
    "tags": [],
    "cell_id": "00032-91d807f0-4d20-4021-994e-4612b3a50623",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4f989e18",
    "execution_start": 1636064553611,
    "execution_millis": 15,
    "allow_embed": "code",
    "cell_id": "00033-1fcaad0d-11e5-4188-aafc-4e9d833e19d2",
    "deepnote_cell_type": "code"
   },
   "source": "from sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline\n\n# Custom transformer class using spaCy\nclass predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        # Implement clean_text\n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\n# Basic function to clean the text\ndef clean_text(text):\n    # Remove spaces and converte text into lowercase\n    return text.strip().lower()\n\n# Bag-of-words data transformation\nfrom sklearn.feature_extraction.text import CountVectorizer\nbow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n\n# Multinomial Naive Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\n\n# Create pipeline\npipe = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', bow_vector),\n                 ('classifier', classifier)])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Model Training\nI first assign the train and test sets of the data and I then train the statistical models (Naive Bayes, Logistic Regression, SVM).",
   "metadata": {
    "tags": [],
    "cell_id": "00034-19e073f7-e68d-46b8-8241-c43d729e40a8",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f8cfb8fc",
    "execution_start": 1636064556855,
    "execution_millis": 4,
    "cell_id": "00035-5d3d18af-ccbd-4d96-85e7-2f892e1e38e7",
    "deepnote_cell_type": "code"
   },
   "source": "X_train = trainDF.OriginalTweet\nX_test = testDF.OriginalTweet\ny_train = trainDF.encoded_sentiment\ny_test = testDF.encoded_sentiment",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "13096d4f",
    "execution_start": 1636064559637,
    "execution_millis": 11176445,
    "allow_embed": "code",
    "cell_id": "00036-7010f183-b024-4cf0-921f-90a661f43f44",
    "deepnote_cell_type": "code"
   },
   "source": "# Multinomial Naive Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\n\n# Create pipeline using Bag of Words\npipe_NB = Pipeline([('cleaner', predictors()),\n                 ('vectorizer', bow_vector),\n                 ('classifier', classifier)])\n\n# model generation\npipe_NB.fit(X_train,y_train)",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 12,
     "data": {
      "text/plain": "Pipeline(steps=[('cleaner', <__main__.predictors object at 0x7f08b80f6990>),\n                ('vectorizer',\n                 CountVectorizer(tokenizer=<function spacy_tokenizer at 0x7f08b4d410e0>)),\n                ('classifier', MultinomialNB())])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f066ca65",
    "execution_start": 1636075736080,
    "execution_millis": 12398729,
    "allow_embed": "code",
    "cell_id": "00037-cbe85be7-6dcc-476e-b81e-64b8aa843dcf",
    "deepnote_cell_type": "code"
   },
   "source": "# Logistic Regression Classifier\nfrom sklearn.linear_model import LogisticRegression\nclassifier_log = LogisticRegression()\n\n# Create pipeline using Bag of Words\npipe_log = Pipeline([('cleaner', predictors()),\n                 ('vectorizer', bow_vector),\n                 ('classifier', classifier_log)])\n\n# model generation\npipe_log.fit(X_train,y_train)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 13,
     "data": {
      "text/plain": "Pipeline(steps=[('cleaner', <__main__.predictors object at 0x7f08ace042d0>),\n                ('vectorizer',\n                 CountVectorizer(tokenizer=<function spacy_tokenizer at 0x7f08b4d410e0>)),\n                ('classifier', LogisticRegression())])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b2ec0915",
    "execution_start": 1636088134814,
    "execution_millis": 12080236,
    "allow_embed": "code",
    "cell_id": "00038-5e24e827-57d9-4f73-81aa-6ebc71baebd1",
    "deepnote_cell_type": "code"
   },
   "source": "# SVM Classifier\nfrom sklearn.svm import SVC\nclassifier_svm = SVC()\n\n# Create pipeline using Bag of Words\npipe_svm = Pipeline([('cleaner', predictors()),\n                 ('vectorizer', bow_vector),\n                 ('classifier', classifier_svm)])\n\n# model generation\npipe_svm.fit(X_train,y_train)",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 14,
     "data": {
      "text/plain": "Pipeline(steps=[('cleaner', <__main__.predictors object at 0x7f08acd3ba10>),\n                ('vectorizer',\n                 CountVectorizer(tokenizer=<function spacy_tokenizer at 0x7f08b4d410e0>)),\n                ('classifier', SVC())])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Model Performance\nI looked at the precision, recall, and F-1 score to measure the accuracy of the model.",
   "metadata": {
    "tags": [],
    "cell_id": "00039-b161718d-2f8d-4675-91f2-ed96703c549e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "79f4bdf0",
    "execution_start": 1636100215051,
    "execution_millis": 2335263,
    "cell_id": "00040-78be061a-fb12-49ff-a10b-ac3c6874d185",
    "allow_embed": "output",
    "deepnote_cell_type": "code"
   },
   "source": "# Classification Report\nfrom sklearn.metrics import classification_report\n\n# Predict with a test dataset\npredicted = pipe_NB.predict(X_test)\n\n# Model Accuracy\nprint(\"Naive Bayes Model:\\n\")\nprint(classification_report(y_test, predicted, target_names = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']))\n\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Naive Bayes Model:\n\n                    precision    recall  f1-score   support\n\nExtremely Negative       0.65      0.23      0.34       490\n          Negative       0.56      0.31      0.40       559\n           Neutral       0.40      0.55      0.46       867\n          Positive       0.67      0.26      0.38       654\nExtremely Positive       0.39      0.64      0.49      1026\n\n          accuracy                           0.44      3596\n         macro avg       0.53      0.40      0.41      3596\n      weighted avg       0.51      0.44      0.43      3596\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4b6f28e9",
    "execution_start": 1636102550303,
    "execution_millis": 2380681,
    "cell_id": "00041-bf1cc838-288d-4283-8bdc-b4674ed29b54",
    "allow_embed": "output",
    "deepnote_cell_type": "code"
   },
   "source": "# Classification Report\nfrom sklearn.metrics import classification_report\n# Predicting with a test dataset\npredicted_log = pipe_log.predict(X_test)\n\n# Model Accuracy\nprint(\"Logistic Regression Model:\\n\")\nprint(classification_report(y_test, predicted_log, target_names = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']))\n\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Logistic Regression Model:\n\n                    precision    recall  f1-score   support\n\nExtremely Negative       0.62      0.55      0.59       490\n          Negative       0.61      0.56      0.58       559\n           Neutral       0.48      0.49      0.49       867\n          Positive       0.57      0.65      0.60       654\nExtremely Positive       0.51      0.51      0.51      1026\n\n          accuracy                           0.54      3596\n         macro avg       0.56      0.55      0.55      3596\n      weighted avg       0.55      0.54      0.54      3596\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "15afd10e",
    "execution_start": 1636105893399,
    "execution_millis": 2396869,
    "cell_id": "00042-318df6e6-3700-4ed4-8396-af84e88a9f34",
    "allow_embed": "output",
    "deepnote_cell_type": "code"
   },
   "source": "# Classificatin Report\nfrom sklearn.metrics import classification_report\n# Predicting with a test dataset\npredicted_svm = pipe_svm.predict(X_test)\n\n# Model Accuracy\nprint(\"SVM Model:\\n\")\nprint(classification_report(y_test, predicted_svm, target_names = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "SVM Model:\n\n                    precision    recall  f1-score   support\n\nExtremely Negative       0.69      0.40      0.51       490\n          Negative       0.71      0.46      0.56       559\n           Neutral       0.48      0.54      0.51       867\n          Positive       0.57      0.68      0.62       654\nExtremely Positive       0.49      0.57      0.52      1026\n\n          accuracy                           0.54      3596\n         macro avg       0.59      0.53      0.54      3596\n      weighted avg       0.56      0.54      0.54      3596\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00031-3c117a3b-34d8-48d6-b09c-df561ad11566",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=36980032-e74f-4047-828e-e2329ad1a610' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "f6655064-7a31-4f42-b139-e372cd302c0f",
  "deepnote_execution_queue": []
 }
}